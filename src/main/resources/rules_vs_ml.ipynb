{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules vs ML\n",
    "All the data was obtained by command:\n",
    "\n",
    "`run RulesExperiment ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_predicates(cell):\n",
    "    ps = pd.read_csv('/home/oleg/Desktop/data/predicates/roadmapepigenomics/\\\n",
    "predicates_{0}.csv'.format(cell), sep=',', na_values=False, low_memory=False, comment='#')\n",
    "    ps = ps.replace(np.nan, False).replace('+', True).replace('-', False)\n",
    "    ps = ps[ps.columns[1:]]\n",
    "    return ps\n",
    "predicates = load_predicates('spleen')\n",
    "predicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rules = pd.read_csv('/home/oleg/Desktop/data/rules/roadmapepigenomics/rules.csv', sep=',', comment='#')\n",
    "rules['id'] = rules['id'].astype('category')\n",
    "rules = rules.sort_values(by=['conviction'], ascending=[0])\n",
    "rules = rules[np.logical_and(rules['conviction'] > 5, rules['support'] > 1000)]\n",
    "rules.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL itself is not that representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Conviction')\n",
    "ax.set_ylabel('KL')\n",
    "ax.set_zlabel('Support')\n",
    "ax.bar(rules['conviction'], rules['support'], rules['KL'], zdir='y', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicates = {}\n",
    "cells = rules['id'].cat.categories\n",
    "for cell in cells:\n",
    "    print('Loading', cell)\n",
    "    ps = load_predicates(cell)\n",
    "    predicates[cell] = ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules mining vs ML\n",
    "\n",
    "Accuracy can be evaluated having error types values, see: https://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "\n",
    "Select rules with high conviction and test them vs ML.\n",
    "\n",
    "Let us investigate dependence on condition, target, support on conviction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rules['FP'] = rules['error_type_1']\n",
    "rules['TP'] = rules['support']\n",
    "rules['FN'] = rules['error_type_2']\n",
    "rules['precision'] = rules['TP'] / (rules['TP'] + rules['FP'])\n",
    "rules['recall'] = rules['TP'] / (rules['TP'] + rules['FN'])\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_zlabel('Conviction')\n",
    "ax.bar(rules['recall'], rules['conviction'], rules['precision'], zdir='y', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules vs ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import re\n",
    "\n",
    "names = [\"Decision Tree\", \"Random Forest\", \"AdaBoost\"]\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier()]\n",
    "\n",
    "headers = [x + \" precision\" for x in names] + [x + \" recall\" for x in names]\n",
    "headers.sort()\n",
    "\n",
    "\n",
    "def allow(p1, p2):\n",
    "    \"\"\"Check if two predicates can be used in a single rule\"\"\"\n",
    "    m1 = re.compile(r'(NO )?([^\\[@]+)(.*)').search(p1).group(2)\n",
    "    m2 = re.compile(r'(NO )?([^\\[@]+)(.*)').search(p2).group(2)\n",
    "    return m1 != m2\n",
    "# Small tests, ingore\n",
    "print(allow('H3K4me3@tss', 'H3K4me3@tes'))\n",
    "print(allow('NO H3K4me3@tss', 'H3K4me3@tes'))\n",
    "print(allow('H3K4me2@tss', 'H3K4me3@tes'))\n",
    "\n",
    "\n",
    "def test_ml(X, target):\n",
    "    Y = X[target]\n",
    "    X = X[X.columns[5:]].drop(target, 1)\n",
    "    ignored = set()\n",
    "    for p in X.columns:\n",
    "        if not allow(p, target):\n",
    "            if p in X.columns:\n",
    "                X = X.drop(p, 1)\n",
    "                ignored.add(p)\n",
    "    \n",
    "    print(\"Ignored {1} => {0}\".format(target, ignored))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.4)\n",
    "\n",
    "    results = {}\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        precision = precision_score(y_test, clf.predict(X_test))\n",
    "        recall = recall_score(y_test, clf.predict(X_test))\n",
    "        results[name + \" precision\"] = precision_score(y_test, clf.predict(X_test))\n",
    "        results[name + \" recall\"] = recall_score(y_test, clf.predict(X_test))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load top rules and analyze accuracy and precision vs ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TOP_RULES = 20\n",
    "results = DataFrame(columns=['index', 'cell', 'condition', 'target', 'conviction', 'precision', 'recall'] + headers)\n",
    "rules2process = rules.head(TOP_RULES)\n",
    "i = 0\n",
    "for n, row in rules2process.iterrows():\n",
    "    cell = row['id']\n",
    "    i+=1\n",
    "    print(\"{0}/{1} {2} => {3}\".format(i, len(rules2process), row['condition_name'], row['target_name']))\n",
    "    ml = test_ml(predicates[cell], row['target_name'])\n",
    "    results.loc[len(results)] = [n, row['id'], row['condition_name'], row['target_name'], \n",
    "                                    row['conviction'], row['precision'], row['recall']] + [ml[x] for x in headers]\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The difference is clear - RM leads to higher precision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data=results[['precision', 'recall'] + headers])\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
